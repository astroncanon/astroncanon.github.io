---
layout: post
title: Multidimensional Gaussian distribution and classification with Gaussians
categories: Math
---

> *by Guido Sanguinetti*

## 1-D Gaussian distribution
1-D Gaussian with zero mean and unit variance ($\mu = 0$, $\sigma^2 = 1$):

![1d gaussian]({{ site.url }}/images/posts/2017-09-04-gaussian.png)

$$
\begin{equation}
p(x|\mu,\sigma^2)=N(x;\mu,\sigma^2)=\frac{1}{\sqrt{2{\pi}\sigma^2}}{\rm exp}[\frac{-(x-\mu)^2}{2\sigma^2}]
\end{equation}
$$

## The multidimensional Gaussian distribution
* The $d$-dimensional vector $X$ is multivariate Gaussian if it has a
probability density function of the following form:[^M]

$$
\begin{equation}
p(X|M,\sum)=\frac{1}{(2\pi)^{d/2}|\sum|^{1/2}}{\rm exp}[-\frac{1}{2}(X-M)^{T}{\sum}^{-1}(X-M)]
\end{equation}
$$

* The pdf is parameterized by the mean vector $M$ and the covariance matrix $\sum$.

* The 1-D Gaussian is a special case of this pdf.

* The argument to the expontial $0.5(X-M)^T{\sum}^{-1}(X-M)$ is referred to as a quadratic form.

## Covariance matrix
* The mean vector $M$ is the expectation of $X$: 

$$
M = E[X]
$$

* The covariance matrix $\sum$ is the expectation of the deviation of $X$ from the mean: 

$$
{\sum} = E[(X-M)(X-M)^T]
$$

* ${\sum}$ is a $d \times d$ symmetric matrix:

$$
\sum_{ij} = E[(X_i-M_i)(X_j-M_j)]=E[(X_j-M_j)(X_i-M_i)]=\sum_{ji}
$$


***
[^M]: M here is capital $\mu$, stands for multidimensional vector.